{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_games = 10000\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.01\n",
    "gamma = 0.95\n",
    "epsilon = 0.01\n",
    "max_t = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Quality_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(5, 5)\n",
    "        self.output = nn.Linear(5, 1)\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action])\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.softmax(self.output(x))\n",
    "        return x\n",
    "\n",
    "class Quality():\n",
    "    def __init__(self):\n",
    "        self.net = Quality_Net()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.lossFunction = nn.MSELoss()\n",
    "    def eval(self, state, action):\n",
    "        return self.net(Variable(torch.FloatTensor(state)), Variable(torch.FloatTensor([action])))\n",
    "    def train(self, state, action, quality):\n",
    "        self.net.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        predicted_quality = self.eval(state, action)\n",
    "        #quality = quality.detach()\n",
    "        quality = Variable(torch.FloatTensor([quality]))\n",
    "        quality.requires_grad = False\n",
    "        loss = self.lossFunction(predicted_quality, quality)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.data[0]\n",
    "    \n",
    "quality_function = Quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(env.action_space.n)\n",
    "    else:\n",
    "        best_action = 0\n",
    "        best_quality = quality_function.eval(state, 0).data[0]\n",
    "        for action in range(env.action_space.n):\n",
    "            quality = quality_function.eval(state, action)\n",
    "            if quality.data[0] > best_quality:\n",
    "                best_quality = quality.data[0]\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "def value(state):\n",
    "    best_action = 0\n",
    "    best_quality = quality_function.eval(state, 0).data[0]\n",
    "    for action in range(env.action_space.n):\n",
    "        quality = quality_function.eval(state, action)\n",
    "        if quality.data[0] > best_quality:\n",
    "            best_quality = quality.data[0]\n",
    "            best_action = action\n",
    "    return best_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattzhang/py3_kernel/lib/python3.5/site-packages/ipykernel/__main__.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Reward 11.0\n",
      "Iteration 100 Reward 8.0\n",
      "Iteration 200 Reward 9.0\n",
      "Iteration 300 Reward 10.0\n",
      "Iteration 400 Reward 10.0\n",
      "Iteration 500 Reward 10.0\n",
      "Iteration 600 Reward 10.0\n",
      "Iteration 700 Reward 10.0\n",
      "Iteration 800 Reward 9.0\n",
      "Iteration 900 Reward 9.0\n",
      "Iteration 1000 Reward 8.0\n",
      "Iteration 1100 Reward 9.0\n",
      "Iteration 1200 Reward 8.0\n",
      "Iteration 1300 Reward 9.0\n",
      "Iteration 1400 Reward 9.0\n",
      "Iteration 1500 Reward 9.0\n",
      "Iteration 1600 Reward 8.0\n",
      "Iteration 1700 Reward 9.0\n",
      "Iteration 1800 Reward 10.0\n",
      "Iteration 1900 Reward 10.0\n",
      "Iteration 2000 Reward 11.0\n",
      "Iteration 2100 Reward 9.0\n",
      "Iteration 2200 Reward 10.0\n",
      "Iteration 2300 Reward 10.0\n",
      "Iteration 2400 Reward 9.0\n",
      "Iteration 2500 Reward 8.0\n",
      "Iteration 2600 Reward 9.0\n",
      "Iteration 2700 Reward 10.0\n",
      "Iteration 2800 Reward 10.0\n",
      "Iteration 2900 Reward 10.0\n",
      "Iteration 3000 Reward 10.0\n",
      "Iteration 3100 Reward 10.0\n",
      "Iteration 3200 Reward 8.0\n",
      "Iteration 3300 Reward 10.0\n",
      "Iteration 3400 Reward 11.0\n",
      "Iteration 3500 Reward 9.0\n",
      "Iteration 3600 Reward 10.0\n",
      "Iteration 3700 Reward 9.0\n",
      "Iteration 3800 Reward 9.0\n",
      "Iteration 3900 Reward 10.0\n",
      "Iteration 4000 Reward 9.0\n",
      "Iteration 4100 Reward 8.0\n",
      "Iteration 4200 Reward 8.0\n",
      "Iteration 4300 Reward 9.0\n",
      "Iteration 4400 Reward 9.0\n",
      "Iteration 4500 Reward 9.0\n",
      "Iteration 4600 Reward 9.0\n",
      "Iteration 4700 Reward 9.0\n",
      "Iteration 4800 Reward 9.0\n",
      "Iteration 4900 Reward 10.0\n",
      "Iteration 5000 Reward 9.0\n",
      "Iteration 5100 Reward 9.0\n",
      "Iteration 5200 Reward 8.0\n",
      "Iteration 5300 Reward 9.0\n",
      "Iteration 5400 Reward 9.0\n",
      "Iteration 5500 Reward 10.0\n",
      "Iteration 5600 Reward 10.0\n",
      "Iteration 5700 Reward 9.0\n",
      "Iteration 5800 Reward 9.0\n",
      "Iteration 5900 Reward 9.0\n",
      "Iteration 6000 Reward 9.0\n",
      "Iteration 6100 Reward 9.0\n",
      "Iteration 6200 Reward 10.0\n",
      "Iteration 6300 Reward 10.0\n",
      "Iteration 6400 Reward 10.0\n",
      "Iteration 6500 Reward 8.0\n",
      "Iteration 6600 Reward 10.0\n",
      "Iteration 6700 Reward 10.0\n",
      "Iteration 6800 Reward 9.0\n",
      "Iteration 6900 Reward 10.0\n",
      "Iteration 7000 Reward 10.0\n",
      "Iteration 7100 Reward 9.0\n",
      "Iteration 7200 Reward 8.0\n",
      "Iteration 7300 Reward 9.0\n",
      "Iteration 7400 Reward 9.0\n",
      "Iteration 7500 Reward 9.0\n",
      "Iteration 7600 Reward 9.0\n",
      "Iteration 7700 Reward 9.0\n",
      "Iteration 7800 Reward 9.0\n",
      "Iteration 7900 Reward 8.0\n",
      "Iteration 8000 Reward 10.0\n",
      "Iteration 8100 Reward 9.0\n",
      "Iteration 8200 Reward 9.0\n",
      "Iteration 8300 Reward 9.0\n",
      "Iteration 8400 Reward 9.0\n",
      "Iteration 8500 Reward 9.0\n",
      "Iteration 8600 Reward 9.0\n",
      "Iteration 8700 Reward 10.0\n",
      "Iteration 8800 Reward 9.0\n",
      "Iteration 8900 Reward 11.0\n",
      "Iteration 9000 Reward 10.0\n",
      "Iteration 9100 Reward 10.0\n",
      "Iteration 9200 Reward 8.0\n",
      "Iteration 9300 Reward 10.0\n",
      "Iteration 9400 Reward 9.0\n",
      "Iteration 9500 Reward 9.0\n",
      "Iteration 9600 Reward 9.0\n",
      "Iteration 9700 Reward 10.0\n",
      "Iteration 9800 Reward 10.0\n",
      "Iteration 9900 Reward 9.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_games):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(max_t):\n",
    "        action = policy(state)\n",
    "        old_state = state\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "        #quality_function.train(old_state, action, (1-lr) * quality_function.eval(old_state, action) + lr * (reward + value(state)))\n",
    "        quality_function.train(old_state, action, reward + value(state))\n",
    "        if (done): break\n",
    "    if i%100 == 0:\n",
    "        print(\"Iteration\", i, \"Reward\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
